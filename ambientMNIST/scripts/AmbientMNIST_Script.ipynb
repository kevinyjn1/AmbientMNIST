{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "from tensorflow.keras import layers, models, optimizers, losses, utils\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# 1) Data prep (same as before) …\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train[..., None].astype(\"float32\")/255.0\n",
        "x_test  = x_test[...,  None].astype(\"float32\")/255.0\n",
        "x_val, x_train = x_train[50000:], x_train[:50000]\n",
        "y_val, y_train = y_train[50000:], y_train[:50000]\n",
        "y_train = utils.to_categorical(y_train, 10)\n",
        "y_val   = utils.to_categorical(y_val,   10)\n",
        "y_test  = utils.to_categorical(y_test,  10)\n",
        "\n",
        "# 2) Model with explicit layer names ------------------------------------------\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(28,28,1)),\n",
        "\n",
        "    layers.Conv2D(16, (5,5), activation=\"relu\", name=\"conv1\"), #24 x 24\n",
        "    layers.MaxPooling2D(2,2), # 12 x 12\n",
        "\n",
        "    layers.Conv2D(16, (5,5), activation=\"relu\", name=\"conv2\"), # 8 x 8\n",
        "    layers.MaxPooling2D(2,2), # 4 x 4 x 16\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(32, activation=\"relu\", name=\"fc1\"),\n",
        "    layers.Dense(10, name=\"fc2\"),\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(1e-3),\n",
        "    loss=losses.CategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "# 3) Train …\n",
        "model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=5, batch_size=50,\n",
        "    validation_data=(x_val, y_val),\n",
        "    verbose=2\n",
        ")\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# 4) Save weights & biases only for layers that have them --------------------\n",
        "def save_layer(name):\n",
        "    layer = model.get_layer(name)\n",
        "    weights = layer.get_weights()\n",
        "    if len(weights) == 2:\n",
        "        w, b = weights\n",
        "        with open(f\"/content/params/{name}_w.param\", \"wb\") as fw: pickle.dump(w, fw)\n",
        "        with open(f\"/content/params/{name}_b.param\", \"wb\") as fb: pickle.dump(b, fb)\n",
        "    else:\n",
        "        print(f\"→ layer '{name}' has no trainable weights.\")\n",
        "\n",
        "for name in [\"conv1\",\"conv2\",\"fc1\",\"fc2\"]:\n",
        "    save_layer(name)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFFsRyaeIYA6",
        "outputId": "2eaddda2-4555-4f82-f1c5-914a78177ca8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1000/1000 - 28s - 28ms/step - accuracy: 0.9174 - loss: 0.2618 - val_accuracy: 0.9743 - val_loss: 0.0883\n",
            "Epoch 2/5\n",
            "1000/1000 - 44s - 44ms/step - accuracy: 0.9762 - loss: 0.0760 - val_accuracy: 0.9811 - val_loss: 0.0641\n",
            "Epoch 3/5\n",
            "1000/1000 - 39s - 39ms/step - accuracy: 0.9825 - loss: 0.0561 - val_accuracy: 0.9831 - val_loss: 0.0583\n",
            "Epoch 4/5\n",
            "1000/1000 - 41s - 41ms/step - accuracy: 0.9851 - loss: 0.0461 - val_accuracy: 0.9846 - val_loss: 0.0501\n",
            "Epoch 5/5\n",
            "1000/1000 - 27s - 27ms/step - accuracy: 0.9881 - loss: 0.0382 - val_accuracy: 0.9865 - val_loss: 0.0480\n",
            "Test Accuracy: 0.9883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ifNM0-TFwIqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def save_first_data(df, save_path='./input.param'):\n",
        "#     first_data = df.iloc[0]\n",
        "#     with open(save_path, 'wb') as f:\n",
        "#         pickle.dump(first_data, f)\n",
        "\n",
        "# temp = x_train[0].reshape(28, 28)\n",
        "# with open('./input.param', 'wb') as f:\n",
        "#     pickle.dump(temp, f)\n",
        "\n",
        "# print(y_train[0])\n",
        "\n",
        "# model.predict(x_train[0].reshape(1,28,28,1))"
      ],
      "metadata": {
        "id": "B8qPtkGLW9r_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6787791-3d6e-480f-d287-780eed1c2f57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ -4.49 ,  -4.723,  -5.584,  10.667, -13.109,  14.655,  -9.066,  -6.806,   1.337,  -1.913]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시: x_test[0] 이미지를 input0.param 으로 저장\n",
        "import pickle\n",
        "idx = 3\n",
        "\n",
        "# x_test는 이미 (num, 28, 28, 1) 형태로 로드되어 있다고 가정\n",
        "sample_img = x_test[idx]  # shape=(28,28,1), 또는 batch 차원 포함하려면 x_test[0:1]\n",
        "\n",
        "with open(\"/content/params/input.param\", \"wb\") as f:\n",
        "    pickle.dump(sample_img, f)\n",
        "\n",
        "print(\"→ input.param에 이미지 저장 완료\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNWT7HrqU3KB",
        "outputId": "e9e1edbd-3af0-4635-a484-92d5c1e3bff9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "→ input.param에 이미지 저장 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbcDQf5aIXID",
        "outputId": "1a1febf7-3b47-4f0c-a9d4-09e8a7b040d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fc2_w (32, 10)\n",
            "fc1_w (256, 32)\n",
            "conv2_w (5, 5, 16, 16)\n",
            "conv1_w (5, 5, 1, 16)\n",
            "input (28, 28, 1)\n",
            "[[  0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   3.   6.   5.  -1.  -5.  -3.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   4.  10.  15.  10.  -2. -12. -11.  -2.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   2.  11.  22.  27.  12. -12. -23. -21.  -6.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   2.   9.  23.  36.  35.   8. -23. -33. -29. -11.  -2.  -1.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   3.   8.  21.  37.  41.  33.   3. -28. -37. -32. -14.  -8.  -5.  -1.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   1.   9.  20.  34.  42.  30.  21.  -1. -24. -29. -27. -17. -14. -15.  -5.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   4.  19.  36.  44.  34.  16.   8.  -8. -18. -19. -18. -18. -20. -23. -14.  -2.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   2.  12.  34.  46.  40.  20.   1.  -6. -14. -14.  -8.  -6. -12. -23. -29. -23.  -8.  -2.   0.   0.]\n",
            " [  0.   0.   0.   1.   6.  22.  46.  46.  26.   4. -14. -18. -20. -14.   4.  11.   3. -16. -31. -28. -17.  -8.   0.   0.]\n",
            " [  0.   0.   0.   1.  13.  34.  49.  39.   7. -13. -26. -29. -23. -12.  11.  26.  20.   3. -24. -34. -27. -16.  -1.   0.]\n",
            " [  0.   0.   0.   2.  21.  41.  47.  27. -15. -29. -33. -27. -18.  -8.  13.  32.  35.  24. -10. -35. -37. -27.  -5.   0.]\n",
            " [  0.   0.   0.   4.  27.  46.  42.  13. -37. -42. -28. -16.  -8.  -3.  10.  29.  43.  40.   3. -36. -45. -33.  -8.   0.]\n",
            " [  0.   0.   0.   5.  30.  49.  35.  -5. -48. -43. -16.  -6.   0.   0.   7.  28.  46.  46.   9. -34. -48. -39.  -9.   0.]\n",
            " [  0.   0.   0.   9.  33.  51.  26. -21. -49. -38.  -6.  -1.   1.   4.  14.  33.  45.  39.   1. -38. -48. -36.  -8.   0.]\n",
            " [  0.   0.   0.  14.  37.  50.  18. -30. -49. -36.   0.   1.   6.  13.  25.  39.  39.  23. -13. -45. -45. -28.  -6.   0.]\n",
            " [  0.   0.   0.  19.  40.  50.  16. -35. -47. -32.   6.   8.  11.  22.  34.  36.  24.   2. -32. -48. -37. -17.  -2.   0.]\n",
            " [  0.   0.   0.  21.  40.  48.  17. -31. -36. -23.   9.  14.  19.  28.  32.  23.   5. -20. -44. -44. -24.  -8.  -1.   0.]\n",
            " [  0.   0.   0.  19.  38.  45.  22. -18. -24. -10.  12.  16.  18.  21.  18.   6. -15. -37. -45. -35. -12.  -2.   0.   0.]\n",
            " [  0.   0.   0.  15.  34.  40.  26.   0.  -7.   1.  11.  12.   9.   6.  -2. -12. -28. -41. -35. -22.  -5.   0.   0.   0.]\n",
            " [  0.   0.   0.   9.  25.  35.  30.  15.   7.  10.  10.   3.  -6. -10. -17. -26. -33. -31. -22. -12.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   3.  16.  26.  29.  24.  13.   9.   1.  -4. -13. -19. -25. -30. -23. -16. -10.  -4.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   1.   7.  14.  21.  22.  17.  11.  -1.  -9. -17. -22. -24. -22. -12.  -5.  -1.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   1.   4.  10.  15.  17.  10.  -2.  -9. -13. -15. -13. -11.  -4.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   2.   5.   8.   6.   0.  -5.  -7.  -5.  -3.  -2.  -1.   0.   0.   0.   0.   0.   0.   0.]]\n",
            "conv1_b (16,)\n",
            "[[  38.   27.   -4.  -43.    0.   40.   12.  -14.]\n",
            " [  68.   24.  -26.  -66.  -24.   43.   40.   -9.]\n",
            " [  74.   -8.  -68.  -92.  -77.   -2.   39.   44.]\n",
            " [  42.  -67. -102. -118.  -88.  -12.   27.   64.]\n",
            " [  -4.  -95. -102.  -68.  -18.    8.    0.   31.]\n",
            " [ -36.  -46.  -28.  -13.   13.    9.  -15.    2.]\n",
            " [ -74.  -35.   11.   25.   25.  -13.  -29.  -25.]\n",
            " [-104.  -58.   -7.    0.  -26.  -64.  -65.  -31.]]\n",
            "conv2_b (16,)\n",
            "fc1_b (32,)\n",
            "fc2_b (10,)\n",
            "Logits: [7.25, -4.9375, -2.5625, -2.34375, -2.03125, -2.125, 2.71875, -2.625, 0.84375, -1.03125]\n",
            "fc2_w (32, 10)\n",
            "fc1_w (256, 32)\n",
            "conv2_w (5, 5, 16, 16)\n",
            "conv1_w (5, 5, 1, 16)\n",
            "input (28, 28, 1)\n",
            "[[  0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   3.   6.   5.  -1.  -5.  -3.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   4.  10.  15.  10.  -2. -12. -11.  -2.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   2.  11.  22.  27.  12. -12. -23. -21.  -6.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   2.   9.  23.  36.  35.   8. -23. -33. -29. -11.  -2.  -1.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   3.   8.  21.  37.  41.  33.   3. -28. -37. -32. -14.  -8.  -5.  -1.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   1.   9.  20.  34.  42.  30.  21.  -1. -24. -29. -27. -17. -14. -15.  -5.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   4.  19.  36.  44.  34.  16.   8.  -8. -18. -19. -18. -18. -20. -23. -14.  -2.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   2.  12.  34.  46.  40.  20.   1.  -6. -14. -14.  -8.  -6. -12. -23. -29. -23.  -8.  -2.   0.   0.]\n",
            " [  0.   0.   0.   1.   6.  22.  46.  46.  26.   4. -14. -18. -20. -14.   4.  11.   3. -16. -31. -28. -17.  -8.   0.   0.]\n",
            " [  0.   0.   0.   1.  13.  34.  49.  39.   7. -13. -26. -29. -23. -12.  11.  26.  20.   3. -24. -34. -27. -16.  -1.   0.]\n",
            " [  0.   0.   0.   2.  21.  41.  47.  27. -15. -29. -33. -27. -18.  -8.  13.  32.  35.  24. -10. -35. -37. -27.  -5.   0.]\n",
            " [  0.   0.   0.   4.  27.  46.  42.  13. -37. -42. -28. -16.  -8.  -3.  10.  29.  43.  40.   3. -36. -45. -33.  -8.   0.]\n",
            " [  0.   0.   0.   5.  30.  49.  35.  -5. -48. -43. -16.  -6.   0.   0.   7.  28.  46.  46.   9. -34. -48. -39.  -9.   0.]\n",
            " [  0.   0.   0.   9.  33.  51.  26. -21. -49. -38.  -6.  -1.   1.   4.  14.  33.  45.  39.   1. -38. -48. -36.  -8.   0.]\n",
            " [  0.   0.   0.  14.  37.  50.  18. -30. -49. -36.   0.   1.   6.  13.  25.  39.  39.  23. -13. -45. -45. -28.  -6.   0.]\n",
            " [  0.   0.   0.  19.  40.  50.  16. -35. -47. -32.   6.   8.  11.  22.  34.  36.  24.   2. -32. -48. -37. -17.  -2.   0.]\n",
            " [  0.   0.   0.  21.  40.  48.  17. -31. -36. -23.   9.  14.  19.  28.  32.  23.   5. -20. -44. -44. -24.  -8.  -1.   0.]\n",
            " [  0.   0.   0.  19.  38.  45.  22. -18. -24. -10.  12.  16.  18.  21.  18.   6. -15. -37. -45. -35. -12.  -2.   0.   0.]\n",
            " [  0.   0.   0.  15.  34.  40.  26.   0.  -7.   1.  11.  12.   9.   6.  -2. -12. -28. -41. -35. -22.  -5.   0.   0.   0.]\n",
            " [  0.   0.   0.   9.  25.  35.  30.  15.   7.  10.  10.   3.  -6. -10. -17. -26. -33. -31. -22. -12.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   3.  16.  26.  29.  24.  13.   9.   1.  -4. -13. -19. -25. -30. -23. -16. -10.  -4.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   1.   7.  14.  21.  22.  17.  11.  -1.  -9. -17. -22. -24. -22. -12.  -5.  -1.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   1.   4.  10.  15.  17.  10.  -2.  -9. -13. -15. -13. -11.  -4.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   2.   5.   8.   6.   0.  -5.  -7.  -5.  -3.  -2.  -1.   0.   0.   0.   0.   0.   0.   0.]]\n",
            "conv1_b (16,)\n",
            "[[  38.   27.   -4.  -43.    0.   40.   12.  -14.]\n",
            " [  68.   24.  -26.  -66.  -24.   43.   40.   -9.]\n",
            " [  74.   -8.  -68.  -92.  -77.   -2.   39.   44.]\n",
            " [  42.  -67. -102. -118.  -88.  -12.   27.   64.]\n",
            " [  -4.  -95. -102.  -68.  -18.    8.    0.   31.]\n",
            " [ -36.  -46.  -28.  -13.   13.    9.  -15.    2.]\n",
            " [ -74.  -35.   11.   25.   25.  -13.  -29.  -25.]\n",
            " [-104.  -58.   -7.    0.  -26.  -64.  -65.  -31.]]\n",
            "conv2_b (16,)\n",
            "fc1_b (32,)\n",
            "fc2_b (10,)\n",
            "Prediction: 0, True label: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "import os\n",
        "import argparse\n",
        "import pickle\n",
        "import numpy as np\n",
        "from scipy import signal\n",
        "\n",
        "file_path = None\n",
        "\n",
        "class Graph:\n",
        "   class Node:\n",
        "      def __init__(self, name, op, a, b):\n",
        "         self.name = name\n",
        "         self.op = op\n",
        "         self.a = a\n",
        "         self.b = b\n",
        "\n",
        "   def __init__(self):\n",
        "      self.nodes = {}\n",
        "\n",
        "   def append(self, name, op, a=None, b=None):\n",
        "      if isinstance(a, str):\n",
        "         a = self.nodes[a]\n",
        "      if isinstance(b, str):\n",
        "         b = self.nodes[b]\n",
        "      self.nodes[name] = Graph.Node(name, op, a, b)\n",
        "\n",
        "   def eval(self, ref):\n",
        "      node = ref\n",
        "      if isinstance(ref, str):\n",
        "         node = self.nodes[ref]\n",
        "\n",
        "      if node.a is None and node.b is None:\n",
        "         return node.op(node.name)\n",
        "      elif node.b is None:\n",
        "         return node.op(node.name, self.eval(node.a))\n",
        "      else:\n",
        "         return node.op(node.name, self.eval(node.a), self.eval(node.b))\n",
        "\n",
        "FIXED_ENABLE = 1\n",
        "F_N = 5\n",
        "F_ONE = 1 << F_N\n",
        "F_K = 1 << (F_N - 1)\n",
        "to_fixed = lambda x: float(int(x * F_ONE))\n",
        "to_fixed_vectorized = np.vectorize(to_fixed)\n",
        "fixed_round = lambda x: float(int(x + F_K) >> F_N)\n",
        "fixed_round_vectorized = np.vectorize(fixed_round)\n",
        "to_float = lambda x: float(x) / F_ONE\n",
        "to_float_vectorized = np.vectorize(to_float)\n",
        "\n",
        "if not FIXED_ENABLE:\n",
        "   identity = lambda x: x\n",
        "   to_fixed_vectorized = np.vectorize(identity)\n",
        "   fixed_round_vectorized = np.vectorize(identity)\n",
        "   to_float_vectorized = np.vectorize(identity)\n",
        "\n",
        "def weight(name):\n",
        "   global file_path\n",
        "   path = os.path.join(file_path, name + '.param')\n",
        "   with open(path, 'rb') as f:\n",
        "      weight = to_fixed_vectorized(pickle.load(f))\n",
        "      print(name + ' ' + str(weight.shape))\n",
        "      return weight;\n",
        "\n",
        "def relu(name, a):\n",
        "   return np.fabs(np.multiply(a, a > 0))\n",
        "\n",
        "def flatten(name, a):\n",
        "   flattened = a.flatten()\n",
        "   rows = flattened.shape[0]\n",
        "   return np.reshape(flattened, [rows, 1])\n",
        "\n",
        "def shuffle_indices(layers, rows, cols):\n",
        "   size = layers * rows * cols\n",
        "   layer_size = rows * cols\n",
        "   idx = 0\n",
        "   offset = 0\n",
        "   count = 0\n",
        "   permutation = [0] * size\n",
        "   while count != size:\n",
        "      if idx > size - layers:\n",
        "         offset += 1\n",
        "         idx = 0\n",
        "      permutation[offset + idx] = count\n",
        "      idx += layers\n",
        "      count += 1\n",
        "   return permutation\n",
        "\n",
        "def shuffle(name, a):\n",
        "   permutation = np.argsort(shuffle_indices(100, 4, 4))\n",
        "   return a[:,permutation]\n",
        "\n",
        "def mul(name, a, b):\n",
        "   return fixed_round_vectorized(np.multiply(a, b))\n",
        "\n",
        "def add(name, a, b):\n",
        "   if len(b.shape) == 1:\n",
        "      b = np.reshape(b, [b.shape[0], 1])\n",
        "   return np.add(a, b)\n",
        "\n",
        "def conv_add(name, a, b):\n",
        "   layers = a.shape[0]\n",
        "   stack = []\n",
        "   for l in range(layers):\n",
        "      stack.append(np.add(a[l], b[l]))\n",
        "   return np.stack(stack, axis=0)\n",
        "\n",
        "def mmul(name, a, b):\n",
        "    # a: (in_dim, units), b: (in_dim, batch)\n",
        "    # we want (units, in_dim) · (in_dim, batch) → (units, batch)\n",
        "    return fixed_round_vectorized(np.dot(a, b))\n",
        "\n",
        "\n",
        "from scipy import signal\n",
        "\n",
        "def conv(name, a, b):\n",
        "   filter = a\n",
        "   layers = filter.shape[0]\n",
        "   stack = []\n",
        "   distrib = len(filter.shape) == len(b.shape)\n",
        "   for l in range(layers):\n",
        "      if distrib:\n",
        "         stack.append(signal.correlate(b[l], filter[l], mode='valid'))\n",
        "      else:\n",
        "         stack.append(signal.correlate(b, filter[l], mode='valid'))\n",
        "   ret = fixed_round_vectorized(np.stack(stack, axis=0))\n",
        "   print(ret[0][0])\n",
        "   return ret;\n",
        "\n",
        "def pooling(mat, ksize, method='max', pad=False):\n",
        "   m, n = mat.shape[:2]\n",
        "   ky,kx = ksize\n",
        "\n",
        "   _ceil = lambda x, y: int(np.ceil(x/float(y)))\n",
        "\n",
        "   if pad:\n",
        "      ny = _ceil(m, ky)\n",
        "      nx = _ceil(n, kx)\n",
        "      size = (ny * ky, nx * kx) + mat.shape[2:]\n",
        "      mat_pad = np.full(size, np.nan)\n",
        "      mat_pad[:m,:n,...] = mat\n",
        "   else:\n",
        "      ny = m // ky\n",
        "      nx = n // kx\n",
        "      mat_pad = mat[:ny * ky, :nx * kx, ...]\n",
        "\n",
        "   new_shape=(ny, ky, nx, kx) + mat.shape[2:]\n",
        "\n",
        "   if method == 'max':\n",
        "      result = np.nanmax(mat_pad.reshape(new_shape), axis=(1,3))\n",
        "   else:\n",
        "      result = np.nanmean(mat_pad.reshape(new_shape), axis=(1,3))\n",
        "\n",
        "   return result\n",
        "\n",
        "def maxpool2x2(name, a):\n",
        "   layers = a.shape[0]\n",
        "   stacks = []\n",
        "   for l in range(layers):\n",
        "      stacks.append(pooling(np.squeeze(a[l]), (2, 2)))\n",
        "   return np.stack(stacks, axis=0)\n",
        "\n",
        "def transpose(name, a):\n",
        "   return a.T\n",
        "\n",
        "def permute(name, a):\n",
        "   ret = np.transpose(a, axes=[3, 2, 0, 1])\n",
        "   return ret\n",
        "\n",
        "def permute_vh(name, a):\n",
        "   return np.transpose(a, axes=[2, 3, 0, 1])\n",
        "\n",
        "def squeeze(name, a):\n",
        "   return np.squeeze(a)\n",
        "\n",
        "def input_reshape(name, a):\n",
        "   return np.reshape(a, [1, 28, 28])\n",
        "\n",
        "def arg_max(name, a):\n",
        "   return np.argmax(a.flatten())\n",
        "\n",
        "def change_c_wh(name, a):\n",
        "   temp = np.reshape(a, [32, 16, 4, 4])\n",
        "  #  와NOTE 32->fc1의 output 개수가 32개\n",
        "  # 16->input feature map의 depth\n",
        "  # 4,4 input feature map 의 height width\n",
        "   # N, C, W, H --> N, W, H, C\n",
        "   temp = np.transpose(temp, axes=[0, 2, 3, 1])\n",
        "   # reshape(32, 256)\n",
        "   return np.reshape(temp, [32, 256])\n",
        "\n",
        "def main():\n",
        "    import os\n",
        "    import numpy as np\n",
        "    from tensorflow.keras.datasets import mnist\n",
        "\n",
        "    global file_path\n",
        "    file_path = '/content/params/'\n",
        "\n",
        "    # # ——— 1) MNIST 테스트 이미지 로드 ———\n",
        "    # (_, _), (x_test, y_test) = mnist.load_data()\n",
        "    # x_test = x_test.astype('float32') / 255.0\n",
        "    # x_test = x_test[..., None]  # (N,28,28,1)\n",
        "    # idx = args.img_index\n",
        "    # img = x_test[idx]           # (28,28,1)\n",
        "    # img_fixed = to_fixed_vectorized(np.squeeze(img))  # (28,28)\n",
        "\n",
        "    # ——— 2) 그래프 구성: Sequential 모델과 1:1 대응 ———\n",
        "    graph = Graph()\n",
        "    # 입력\n",
        "    graph.append('input', weight)\n",
        "    graph.append('input_reshape', input_reshape, 'input')\n",
        "\n",
        "    # conv1 → bias → relu → pool\n",
        "    graph.append('conv1_w', weight)\n",
        "    graph.append('conv1_w_', permute, 'conv1_w')\n",
        "    graph.append('conv1_b', weight)\n",
        "    graph.append('conv1',    conv,       'conv1_w_',   'input_reshape')\n",
        "    graph.append('conv1b',   conv_add,   'conv1',     'conv1_b')\n",
        "    graph.append('conv1r',   relu,       'conv1b')\n",
        "    graph.append('conv1p',   maxpool2x2, 'conv1r')\n",
        "\n",
        "    # conv2 → bias → relu → pool\n",
        "    graph.append('conv2_w', weight)\n",
        "    graph.append('conv2_w_', permute, 'conv2_w')\n",
        "    graph.append('conv2_b', weight)\n",
        "    graph.append('conv2',    conv,       'conv2_w_',   'conv1p')\n",
        "    graph.append('conv2b',   conv_add,   'conv2',     'conv2_b')\n",
        "    graph.append('conv2r',   relu,       'conv2b')\n",
        "    graph.append('conv2p',   maxpool2x2, 'conv2r')\n",
        "\n",
        "    # flatten\n",
        "    graph.append('flat',     flatten,    'conv2p')\n",
        "\n",
        "    # fc1 → bias → relu\n",
        "    graph.append('fc1_w',    weight)\n",
        "    graph.append('fc1_b',    weight)\n",
        "    graph.append('fc1_wt', transpose, 'fc1_w')\n",
        "    graph.append('fc1_whc', change_c_wh, 'fc1_wt')\n",
        "    graph.append('fc1',      mmul,       'fc1_whc',     'flat')\n",
        "    graph.append('fc1b',     add,        'fc1',       'fc1_b')\n",
        "    graph.append('fc1r',     relu,       'fc1b')\n",
        "\n",
        "    # fc2 → bias → logits → prediction\n",
        "    graph.append('fc2_w',    weight)\n",
        "    graph.append('fc2_wt', transpose, 'fc2_w')\n",
        "    graph.append('fc2_b',    weight)\n",
        "    graph.append('fc2',      mmul,       'fc2_wt',     'fc1r')\n",
        "    graph.append('fc2b',     add,        'fc2',       'fc2_b')\n",
        "    graph.append('predict',  arg_max,    'fc2b')\n",
        "\n",
        "    # ——— 3) 실행 및 출력 ———\n",
        "    np.set_printoptions(precision=3, linewidth=200, suppress=True)\n",
        "    logits = graph.eval('fc2b').flatten()\n",
        "    print('Logits:', to_float_vectorized(logits).tolist())\n",
        "    pred   = graph.eval('predict')\n",
        "    print(f'Prediction: {pred}, True label: {y_test[idx]}')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "   ##parser = argparse.ArgumentParser()\n",
        "   #parser.add_argument('--src_dir', type=str, help='Source directory')\n",
        "   #args = parser.parse_args()\n",
        "   main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import argparse\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "param_dir = None\n",
        "header_dir = None\n",
        "\n",
        "f_lit = lambda x: 'F_LIT(' + str(x) + ')'\n",
        "\n",
        "def write_header(name, mats):\n",
        "   contents = '#ifndef ' + name.upper() + '_H\\n'\n",
        "   contents += '#define ' + name.upper() + '_H\\n'\n",
        "   contents += '#include \\'<libfixed/fixed.h>\\'\\n'\n",
        "   contents += '#include \\'<libdnn/mem.h>\\'\\n\\n'\n",
        "   for mat_name, mat, layer, sparse in mats:\n",
        "      if layer == 'CONV' and sparse:\n",
        "         mat_str = ''\n",
        "         offsets_str = ''\n",
        "         sizes_str = ''\n",
        "         size = 0\n",
        "         mat = mat.reshape(mat.shape[0], -1)\n",
        "         for m in mat:\n",
        "            data = m[m != 0.0].astype(dtype=str)\n",
        "\n",
        "            idx = np.where(m != 0.0)[0]\n",
        "            offsets = np.diff(idx).flatten()\n",
        "            if data.shape[0] > 0:\n",
        "               data_size = data.flatten().shape[0]\n",
        "               str_mat = str(map(f_lit, data.flatten().tolist()))\n",
        "               mat_str += str_mat.replace('[', '').replace(']', '') + ','\n",
        "\n",
        "               str_offsets = str([idx[0]] + offsets.flatten().tolist())\n",
        "               offsets_str += str_offsets.replace('[', '').replace(']', '') + ','\n",
        "\n",
        "               sizes_str += str(data_size) + ','\n",
        "               size += data_size\n",
        "            else:\n",
        "               sizes_str += '0,'\n",
        "\n",
        "         mat_str = mat_str[:-1]\n",
        "         offsets_str = offsets_str[:-1]\n",
        "         sizes_str = sizes_str[:-1]\n",
        "         layers = mat.shape[0]\n",
        "\n",
        "         contents += '#define ' + mat_name.upper() + '_LEN ' + str(size) + '\\n\\n'\n",
        "\n",
        "         contents += '__ro_hifram fixed ' + mat_name + \\\n",
        "            '[' + str(size) + '] = {' + mat_str + '};\\n\\n'\n",
        "\n",
        "         contents += '__ro_hifram fixed ' + mat_name + '_offsets[' + \\\n",
        "            str(size) + '] = {' + offsets_str + '};\\n\\n'\n",
        "\n",
        "         contents += '__ro_hifram fixed ' + mat_name + '_sizes[' + \\\n",
        "            str(layers) + '] = {' + sizes_str + '};\\n\\n'\n",
        "\n",
        "      elif layer == 'FC' and sparse:\n",
        "         csr = scipy.sparse.csr_matrix(mat)\n",
        "         data, indices, indptr = csr.data, csr.indices, csr.indptr\n",
        "         mat_str = str(map(f_lit, data.flatten().tolist()))\n",
        "         mat_str = mat_str.replace('[', '{').replace(']', '}')\n",
        "         indices_str = str(indices.flatten().tolist())\n",
        "         indices_str = indices_str.replace('[', '{').replace(']', '}')\n",
        "         indptr_str = str(indptr.flatten().tolist())\n",
        "         indptr_str = indptr_str.replace('[', '{').replace(']', '}')\n",
        "\n",
        "         contents += '#define ' + mat_name.upper() + '_LEN ' + \\\n",
        "            str(len(data)) + '\\n\\n'\n",
        "\n",
        "         contents += '__ro_hifram fixed ' + mat_name + '[' + \\\n",
        "            str(len(data)) + '] = ' + mat_str + ';\\n\\n'\n",
        "\n",
        "         contents += '__ro_hifram uint16_t ' + mat_name + '_offsets[' + \\\n",
        "            str(len(indices)) + '] = ' + indices_str + ';\\n\\n'\n",
        "\n",
        "         contents += '__ro_hifram uint16_t ' + mat_name + '_sizes[' + \\\n",
        "            str(len(indptr)) + '] = ' + indptr_str + ';\\n\\n'\n",
        "      else:\n",
        "          # … your existing setup …\n",
        "          flat = mat.flatten().tolist()\n",
        "          # build a list of \"F_LIT(x)\" strings, coercing exact zeros to integer 0 if you like:\n",
        "          lits = [f'F_LIT({int(x)})' if x == 0.0 else f'F_LIT({x})' for x in flat]\n",
        "          mat_str = '{ ' + ', '.join(lits) + ' }'\n",
        "          shape_str = ''.join(f'[{s}]' for s in mat.shape)\n",
        "\n",
        "          contents += f'__ro_hifram fixed {mat_name}{shape_str} = {mat_str};\\n\\n'\n",
        "        #  print(mat.flatten().tolist())\n",
        "        #  mat_str = str(map(f_lit, mat.flatten().tolist()))\n",
        "        #  mat_str = mat_str.replace('[', '{').replace(']', '}')\n",
        "        #  shape_str = ''\n",
        "        #  for s in mat.shape:\n",
        "        #     shape_str += '[' + str(s) + ']'\n",
        "\n",
        "        #  contents += '__ro_hifram fixed ' + mat_name + \\\n",
        "        #     shape_str + ' = ' + mat_str + ';\\n\\n'\n",
        "\n",
        "   contents = contents.replace(\"'\", '')\n",
        "   contents += '#endif'\n",
        "   path = os.path.join(header_dir, name + '.h')\n",
        "   with open(path, 'w+') as f:\n",
        "      f.write(contents)\n",
        "\n",
        "def weight(name):\n",
        "   global param_dir\n",
        "   path = os.path.join(param_dir, name + '.param')\n",
        "   with open(path, 'rb') as f:\n",
        "      data = pickle.load(f)\n",
        "      return data\n",
        "\n",
        "def main():\n",
        "   global header_dir, param_dir\n",
        "   header_dir = '/content/headers'\n",
        "   param_dir = '/content/params'\n",
        "\n",
        "   graph = Graph()\n",
        "\n",
        "   graph.append('input', weight)\n",
        "   graph.append('input_reshape', input_reshape, 'input')\n",
        "\n",
        "   graph.append('conv1_w', weight)\n",
        "   graph.append('conv1_wp', permute, 'conv1_w')\n",
        "   graph.append('conv1_b', weight)\n",
        "\n",
        "   graph.append('conv2_w', weight)\n",
        "   graph.append('conv2_wp', permute, 'conv2_w')\n",
        "   graph.append('conv2_b', weight)\n",
        "\n",
        "   graph.append('fc1_w', weight)\n",
        "   graph.append('fc1_wt', transpose, 'fc1_w')\n",
        "   graph.append('fc1_whc', change_c_wh, 'fc1_wt')\n",
        "   graph.append('fc1_b', weight)\n",
        "\n",
        "   graph.append('fc2_w', weight)\n",
        "   graph.append('fc2_wt', transpose, 'fc2_w')\n",
        "   graph.append('fc2_b', weight)\n",
        "\n",
        "   write_header('input', [\n",
        "      ('input', graph.eval('input_reshape'), 'FC', False)])\n",
        "\n",
        "   write_header('conv1', [\n",
        "      ('conv1_w', graph.eval('conv1_wp'), 'CONV', False),\n",
        "      ('conv1_b', graph.eval('conv1_b'), 'FC', False)])\n",
        "\n",
        "   write_header('conv2', [\n",
        "      ('conv2_w', graph.eval('conv2_wp'), 'CONV', False),\n",
        "      ('conv2_b', graph.eval('conv2_b'), 'FC', False)])\n",
        "\n",
        "   write_header('fc1', [\n",
        "      ('fc1_w', graph.eval('fc1_whc'), 'FC', False),\n",
        "      ('fc1_b', graph.eval('fc1_b'), 'FC', False)])\n",
        "\n",
        "   write_header('fc2', [\n",
        "      ('fc2_w', graph.eval('fc2_wt'), 'FC', False),\n",
        "      ('fc2_b', graph.eval('fc2_b'), 'FC', False)])\n",
        "\n",
        "  #  write_header('fc1_sparse', [\n",
        "  #     ('fc1_w', graph.eval('fc1_whc'), 'FC', True),\n",
        "  #     ('fc1_b', graph.eval('fc1_b'), 'FC', False)])\n",
        "\n",
        "  #  write_header('fc2_sparse', [\n",
        "  #     ('fc2_w', graph.eval('fc2_wt'), 'FC', True),\n",
        "  #     ('fc2_b', graph.eval('fc2_b'), 'FC', False)])\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "   main()\n",
        "\n"
      ],
      "metadata": {
        "id": "ajajB-xAKdLL"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}